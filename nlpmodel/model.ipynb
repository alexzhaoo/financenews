{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/VSCodeProjects/financenews/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def tokenization(inputpath):\n",
    "\n",
    "    df = pd.read_csv(inputpath)\n",
    "\n",
    "    df['sentence'] = df['sentence'].astype(str)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "    model = AutoModel.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "    tokens = tokenizer(df['sentence'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**tokens).last_hidden_state.mean(dim=1) \n",
    "\n",
    "    labels = torch.tensor(df['Label'])\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(inputpath):\n",
    "    embeddings, labels = tokenization(inputpath)\n",
    "    X = embeddings.numpy()\n",
    "    y = labels.numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data('../cleaning/concatenated.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN,self).__init__()\n",
    "        self.fc2 = torch.nn.Linear(768, 512)\n",
    "        self.fc3 = torch.nn.Linear(512, 256)\n",
    "        self.fc4 = torch.nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.7024738192558289\n",
      "Epoch 5, Loss 0.09602687507867813\n",
      "Epoch 10, Loss 0.07050268352031708\n",
      "Epoch 15, Loss 0.05883525311946869\n",
      "Epoch 20, Loss 0.039825353771448135\n",
      "Epoch 25, Loss 0.030922414734959602\n",
      "Epoch 30, Loss 0.024480929598212242\n",
      "Epoch 35, Loss 0.018110984936356544\n",
      "Epoch 40, Loss 0.013544993475079536\n",
      "Epoch 45, Loss 0.009159705601632595\n",
      "Epoch 50, Loss 0.005838892888277769\n",
      "Epoch 55, Loss 0.0034598554484546185\n",
      "Epoch 60, Loss 0.0020118679385632277\n",
      "Epoch 65, Loss 0.0012570340186357498\n",
      "Epoch 70, Loss 0.0008497158414684236\n",
      "Epoch 75, Loss 0.0006181419012136757\n",
      "Epoch 80, Loss 0.0004804849158972502\n",
      "Epoch 85, Loss 0.0003948535886593163\n",
      "Epoch 90, Loss 0.00033835184876807034\n",
      "Epoch 95, Loss 0.0002999391872435808\n"
     ]
    }
   ],
   "source": [
    "ANN = ANN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ANN.parameters(), lr=0.001 , weight_decay=0.0001)\n",
    "\n",
    "def trainclassifier(classifier, X_train, y_train):\n",
    "\n",
    "    classifier.train()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        inputs = torch.Tensor(torch.Tensor(X_train).float())\n",
    "        labels = torch.Tensor(torch.Tensor(y_train).long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ANN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch {}, Loss {}'.format(epoch,loss.item()))\n",
    "\n",
    "    return ANN\n",
    "\n",
    "classifier = trainclassifier(ANN, X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
